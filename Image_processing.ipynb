{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image processing",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmit22/LifeHack-2020/blob/master/Image_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TYbFSrULqtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import imutils"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB7XxAOMPE84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d31cdc16-4724-409b-d635-3aa7b04dd64f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file = open(\"/content/drive/My Drive/LifeHack/data.txt\", \"w\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiomMaM1uyb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "4e16c180-6fa5-4a53-dd2f-18431d41f6f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_IR9FOHA3eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir = os.path.join(os.getcwd(), \"drive/My Drive/data\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkA7w0YLMARY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadData(dir):\n",
        "\n",
        "    images = []\n",
        "    category = []\n",
        "    file_dir = []\n",
        "\n",
        "    for img in os.listdir(dir):\n",
        "\n",
        "        if img.endswith(\".png\"):\n",
        "        #images.append(cv2.imread(os.path.join(dir, img)))\n",
        "            file_dir.append(os.path.join(dir, img))\n",
        "\n",
        "            if img.startswith('NL'):      #assigning label to file according to          \n",
        "                category.append(0)             # their type\n",
        "\n",
        "            elif img.startswith('ca'):    \n",
        "                category.append(1)\n",
        "\n",
        "            elif img.startswith('Gl'):  \n",
        "                category.append(2)\n",
        "\n",
        "            elif img.startswith('Re'):          \n",
        "                category.append(3)\n",
        "\n",
        "    file_dir = np.array(file_dir)\n",
        "    category = np.array(category, dtype = 'int32')  \n",
        "\n",
        "    return shuffle(file_dir, category,random_state=817328462)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu9nYwnu6pGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_dir, category = loadData(dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvy05Ni1VnVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "598fed77-fd01-4305-f436-4c2f68cc1ae6"
      },
      "source": [
        "print(len(file_dir))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-buEv2QNg3CS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(encoded_val):\n",
        "      \n",
        "      labels = {0:'normal', 1:'cataract', 2:'glaucoma', 3:'retina'}\n",
        "      return labels[class_code]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNg0Ccm9_Hzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_augment(file_dir, labels):\n",
        "\n",
        "    img_set = []\n",
        "    lbl_set = []\n",
        "\n",
        "    for i in range(len(file_dir)):\n",
        "\n",
        "        if i%100==0:\n",
        "            print(i)\n",
        "        label = labels[i]\n",
        "        img = get_object(file_dir[i])\n",
        "\n",
        "        img_b = img + 0.05*img\n",
        "        img_d = img - 0.05*img\n",
        "\n",
        "        flip_v_b = cv2.flip(img_b,0)\n",
        "        flip_h_b = cv2.flip(img_b,1)\n",
        "\n",
        "        flip_v_d = cv2.flip(img_d,0)\n",
        "        flip_h_d = cv2.flip(img_d,1)\n",
        "\n",
        "        flip_v = cv2.flip(img,0) #vertical flip\n",
        "        flip_h = cv2.flip(img,1) #lr flip\n",
        "\n",
        "        img_set.append(img)\n",
        "        # img_set.append(img_b)\n",
        "        # img_set.append(img_d)\n",
        "        img_set.append(flip_v)\n",
        "        img_set.append(flip_h)\n",
        "        img_set.append(flip_v_b)\n",
        "        img_set.append(flip_h_b)\n",
        "        img_set.append(flip_v_d)\n",
        "        img_set.append(flip_h_d)\n",
        "\n",
        "        # lbl_set.append(label)\n",
        "        # lbl_set.append(label)\n",
        "        lbl_set.append(label)\n",
        "        lbl_set.append(label)\n",
        "        lbl_set.append(label)\n",
        "        lbl_set.append(label)\n",
        "        lbl_set.append(label)\n",
        "        lbl_set.append(label)\n",
        "        lbl_set.append(label)\n",
        "    print('Done')\n",
        "    return np.array(img_set),np.array(lbl_set)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM9v6y0RLq51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_object(filename):\n",
        "\n",
        "    image = cv2.imread(filename)\n",
        "    im_size = 256\n",
        "    \n",
        "    gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) # convert 2 grayscale\n",
        "\n",
        "    retval, threshold = cv2.threshold(gray,10,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    contours, hierarchy = cv2.findContours(threshold,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) # find contours\n",
        "\n",
        "    # ensure at least some circles were found\n",
        "    if contours:\n",
        "\n",
        "        contours = sorted(contours, key=cv2.contourArea, reverse=True) \n",
        "\n",
        "        #find the bounding rect\n",
        "        x,y,w,h = cv2.boundingRect(contours[0])                  \n",
        "        img = image[y:y+h,x:x+w]# crop image\n",
        "\n",
        "        resize=cv2.resize(img,(im_size,im_size)) # resize to im_size X im_size size\n",
        "        \n",
        "        return resize\n",
        "\n",
        "    return cv2.resize(image, (im_size,im_size))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24dbI6tV4Cn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "37150a33-e8c7-4390-8579-900fcff266b3"
      },
      "source": [
        "X, y = data_augment(file_dir, category)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N943zmIr9X31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df5acb79-52c4-4700-d5da-06af599832e1"
      },
      "source": [
        "print(len(X))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LiOmwf8-Rkp",
        "colab_type": "text"
      },
      "source": [
        "Rotates the images "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2tfc0g5wDpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f47f36d-a475-4f4b-a63b-b310543bd2fc"
      },
      "source": [
        "import keras\n",
        "y = keras.utils.to_categorical(y, 4)\n",
        "X = X.astype('float32')\n",
        "X /= 255"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rPrNyykvo_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJpdE2Y80LB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import keras"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yspEovdEwE86",
        "colab_type": "text"
      },
      "source": [
        "Creating model using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaoA4DFlOfl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b6502c0-c018-45ec-d368-c0a286ee8bf5"
      },
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "#number of filters(patterns to be detected), window size \n",
        "model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(256,256,3)))\n",
        "model.add(Conv2D(32, (3,3), activation='relu'))\n",
        "\n",
        "#Scales down, by discarding less useful data\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "#Prevents memorizing of datapoint sets\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "##compile the model\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#train the model\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=32,\n",
        "    epochs=20, #number of iterations\n",
        "    validation_data=(X_test, y_test),\n",
        "    shuffle=True     \n",
        ")\n",
        "# from pathlib import Path\n",
        "# #save the neural network structure\n",
        "# model_structure = model.to_json()\n",
        "# f = Path(\"model_structure.json\")\n",
        "# f.write_text(model_structure)\n",
        "\n",
        "# #Save the network's trained weights\n",
        "# model.save_weights(\"model_weights.h5\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 256, 256, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 254, 254, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 127, 127, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 127, 127, 32)      0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 516128)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               66064512  \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 4)                 516       \n",
            "=================================================================\n",
            "Total params: 66,091,684\n",
            "Trainable params: 66,091,684\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 3155 samples, validate on 1052 samples\n",
            "Epoch 1/20\n",
            "3155/3155 [==============================] - 35s 11ms/step - loss: 2.6850 - accuracy: 0.4634 - val_loss: 1.2475 - val_accuracy: 0.5010\n",
            "Epoch 2/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.2549 - accuracy: 0.4976 - val_loss: 1.2474 - val_accuracy: 0.5010\n",
            "Epoch 3/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.2513 - accuracy: 0.4983 - val_loss: 1.2302 - val_accuracy: 0.5010\n",
            "Epoch 4/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.2248 - accuracy: 0.4995 - val_loss: 1.2517 - val_accuracy: 0.5010\n",
            "Epoch 5/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.2131 - accuracy: 0.5036 - val_loss: 1.2024 - val_accuracy: 0.5133\n",
            "Epoch 6/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.2007 - accuracy: 0.5062 - val_loss: 1.1639 - val_accuracy: 0.5219\n",
            "Epoch 7/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.1656 - accuracy: 0.5173 - val_loss: 1.1448 - val_accuracy: 0.5323\n",
            "Epoch 8/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.1557 - accuracy: 0.5306 - val_loss: 1.1434 - val_accuracy: 0.5361\n",
            "Epoch 9/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.1312 - accuracy: 0.5388 - val_loss: 1.1375 - val_accuracy: 0.5333\n",
            "Epoch 10/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.1044 - accuracy: 0.5544 - val_loss: 1.1074 - val_accuracy: 0.5504\n",
            "Epoch 11/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 1.0320 - accuracy: 0.5854 - val_loss: 0.9682 - val_accuracy: 0.6112\n",
            "Epoch 12/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.9324 - accuracy: 0.6269 - val_loss: 0.9061 - val_accuracy: 0.6302\n",
            "Epoch 13/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.9355 - accuracy: 0.6323 - val_loss: 0.8672 - val_accuracy: 0.6597\n",
            "Epoch 14/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.7215 - accuracy: 0.7192 - val_loss: 0.7383 - val_accuracy: 0.7072\n",
            "Epoch 15/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.5916 - accuracy: 0.7826 - val_loss: 0.6659 - val_accuracy: 0.7766\n",
            "Epoch 16/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.5091 - accuracy: 0.8200 - val_loss: 0.6282 - val_accuracy: 0.7918\n",
            "Epoch 17/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.4333 - accuracy: 0.8472 - val_loss: 0.5502 - val_accuracy: 0.8137\n",
            "Epoch 18/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.3501 - accuracy: 0.8862 - val_loss: 0.6289 - val_accuracy: 0.7871\n",
            "Epoch 19/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.3278 - accuracy: 0.8964 - val_loss: 0.5460 - val_accuracy: 0.8127\n",
            "Epoch 20/20\n",
            "3155/3155 [==============================] - 27s 9ms/step - loss: 0.2553 - accuracy: 0.9166 - val_loss: 0.6421 - val_accuracy: 0.8413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fa3f02ea9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFatuPTRy8Tu",
        "colab_type": "text"
      },
      "source": [
        "Model 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvZGCJnCPtzr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04ccde52-809e-4756-9d7d-3502cd5f73bf"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "# create the base pre-trained model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# and a logistic layer -- let's say we have 200 classes\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional InceptionV3 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# train the model on the new data for a few epochs\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=16,\n",
        "    epochs=50, #number of iterations\n",
        "    validation_data=(X_test, y_test),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# # at this point, the top layers are well trained and we can start fine-tuning\n",
        "# # convolutional layers from inception V3. We will freeze the bottom N layers\n",
        "# # and train the remaining top layers.\n",
        "\n",
        "# # let's visualize layer names and layer indices to see how many layers\n",
        "# # we should freeze:\n",
        "# for i, layer in enumerate(base_model.layers):\n",
        "#    print(i, layer.name)\n",
        "\n",
        "# # we chose to train the top 2 inception blocks, i.e. we will freeze\n",
        "# # the first 249 layers and unfreeze the rest:\n",
        "# for layer in model.layers[:249]:\n",
        "#    layer.trainable = False\n",
        "# for layer in model.layers[249:]:\n",
        "#    layer.trainable = True\n",
        "\n",
        "# # we need to recompile the model for these modifications to take effect\n",
        "# # we use SGD with a low learning rate\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "# model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
        "\n",
        "# # we train our model again (this time fine-tuning the top 2 inception blocks\n",
        "# # alongside the top Dense layers\n",
        "# model.fit(...)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "85/85 [==============================] - 15s 181ms/step - loss: 2.0221 - accuracy: 0.4771 - val_loss: 1.2565 - val_accuracy: 0.6430\n",
            "Epoch 2/50\n",
            "85/85 [==============================] - 14s 164ms/step - loss: 1.0797 - accuracy: 0.5754 - val_loss: 1.0438 - val_accuracy: 0.5898\n",
            "Epoch 3/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.9528 - accuracy: 0.6176 - val_loss: 1.0967 - val_accuracy: 0.5987\n",
            "Epoch 4/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.8913 - accuracy: 0.6220 - val_loss: 0.9514 - val_accuracy: 0.6319\n",
            "Epoch 5/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.8338 - accuracy: 0.6723 - val_loss: 0.8784 - val_accuracy: 0.6386\n",
            "Epoch 6/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.7916 - accuracy: 0.6812 - val_loss: 0.8818 - val_accuracy: 0.6386\n",
            "Epoch 7/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.7572 - accuracy: 0.6857 - val_loss: 0.9570 - val_accuracy: 0.6652\n",
            "Epoch 8/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.7070 - accuracy: 0.7130 - val_loss: 0.9212 - val_accuracy: 0.6563\n",
            "Epoch 9/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.6683 - accuracy: 0.7330 - val_loss: 0.8501 - val_accuracy: 0.6563\n",
            "Epoch 10/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.6414 - accuracy: 0.7396 - val_loss: 0.9485 - val_accuracy: 0.6630\n",
            "Epoch 11/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.5758 - accuracy: 0.7700 - val_loss: 0.8959 - val_accuracy: 0.6452\n",
            "Epoch 12/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.5485 - accuracy: 0.7781 - val_loss: 1.1459 - val_accuracy: 0.6364\n",
            "Epoch 13/50\n",
            "85/85 [==============================] - 14s 164ms/step - loss: 0.4875 - accuracy: 0.7981 - val_loss: 1.3205 - val_accuracy: 0.5211\n",
            "Epoch 14/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.4847 - accuracy: 0.8055 - val_loss: 0.9278 - val_accuracy: 0.6851\n",
            "Epoch 15/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.4484 - accuracy: 0.8232 - val_loss: 0.8518 - val_accuracy: 0.6940\n",
            "Epoch 16/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.4127 - accuracy: 0.8351 - val_loss: 0.9487 - val_accuracy: 0.6541\n",
            "Epoch 17/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.3842 - accuracy: 0.8462 - val_loss: 1.0178 - val_accuracy: 0.6475\n",
            "Epoch 18/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.3586 - accuracy: 0.8587 - val_loss: 0.9438 - val_accuracy: 0.6585\n",
            "Epoch 19/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.3351 - accuracy: 0.8713 - val_loss: 1.0619 - val_accuracy: 0.7029\n",
            "Epoch 20/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.3110 - accuracy: 0.8794 - val_loss: 1.1752 - val_accuracy: 0.6741\n",
            "Epoch 21/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.2749 - accuracy: 0.8950 - val_loss: 2.0895 - val_accuracy: 0.4235\n",
            "Epoch 22/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.2639 - accuracy: 0.8987 - val_loss: 0.9919 - val_accuracy: 0.6896\n",
            "Epoch 23/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.2348 - accuracy: 0.9149 - val_loss: 1.0295 - val_accuracy: 0.7118\n",
            "Epoch 24/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.2051 - accuracy: 0.9157 - val_loss: 1.4127 - val_accuracy: 0.5854\n",
            "Epoch 25/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.2043 - accuracy: 0.9290 - val_loss: 1.5778 - val_accuracy: 0.5698\n",
            "Epoch 26/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.1818 - accuracy: 0.9297 - val_loss: 1.3847 - val_accuracy: 0.6231\n",
            "Epoch 27/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.1703 - accuracy: 0.9371 - val_loss: 1.3472 - val_accuracy: 0.6608\n",
            "Epoch 28/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.1555 - accuracy: 0.9438 - val_loss: 1.1564 - val_accuracy: 0.7140\n",
            "Epoch 29/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.1273 - accuracy: 0.9527 - val_loss: 1.4126 - val_accuracy: 0.6874\n",
            "Epoch 30/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.1269 - accuracy: 0.9527 - val_loss: 1.4020 - val_accuracy: 0.7007\n",
            "Epoch 31/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.1208 - accuracy: 0.9490 - val_loss: 1.1961 - val_accuracy: 0.7051\n",
            "Epoch 32/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.1189 - accuracy: 0.9608 - val_loss: 1.6436 - val_accuracy: 0.6874\n",
            "Epoch 33/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.1223 - accuracy: 0.9660 - val_loss: 1.5029 - val_accuracy: 0.6785\n",
            "Epoch 34/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.1219 - accuracy: 0.9586 - val_loss: 1.1382 - val_accuracy: 0.7007\n",
            "Epoch 35/50\n",
            "85/85 [==============================] - 14s 163ms/step - loss: 0.0953 - accuracy: 0.9689 - val_loss: 1.2238 - val_accuracy: 0.6962\n",
            "Epoch 36/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0816 - accuracy: 0.9778 - val_loss: 1.2225 - val_accuracy: 0.7118\n",
            "Epoch 37/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0738 - accuracy: 0.9808 - val_loss: 1.4880 - val_accuracy: 0.6785\n",
            "Epoch 38/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0895 - accuracy: 0.9741 - val_loss: 1.3866 - val_accuracy: 0.7029\n",
            "Epoch 39/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0754 - accuracy: 0.9771 - val_loss: 1.4090 - val_accuracy: 0.7140\n",
            "Epoch 40/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.1211 - accuracy: 0.9638 - val_loss: 1.4455 - val_accuracy: 0.7162\n",
            "Epoch 41/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0679 - accuracy: 0.9822 - val_loss: 1.3750 - val_accuracy: 0.7162\n",
            "Epoch 42/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0788 - accuracy: 0.9800 - val_loss: 1.3934 - val_accuracy: 0.7118\n",
            "Epoch 43/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0686 - accuracy: 0.9837 - val_loss: 1.3832 - val_accuracy: 0.7029\n",
            "Epoch 44/50\n",
            "85/85 [==============================] - 14s 161ms/step - loss: 0.0851 - accuracy: 0.9749 - val_loss: 1.5050 - val_accuracy: 0.7118\n",
            "Epoch 45/50\n",
            "85/85 [==============================] - 14s 161ms/step - loss: 0.0708 - accuracy: 0.9786 - val_loss: 1.8885 - val_accuracy: 0.7118\n",
            "Epoch 46/50\n",
            "85/85 [==============================] - 14s 161ms/step - loss: 0.0382 - accuracy: 0.9904 - val_loss: 1.5766 - val_accuracy: 0.6896\n",
            "Epoch 47/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0765 - accuracy: 0.9741 - val_loss: 1.5670 - val_accuracy: 0.7184\n",
            "Epoch 48/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0453 - accuracy: 0.9904 - val_loss: 1.5032 - val_accuracy: 0.7184\n",
            "Epoch 49/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0919 - accuracy: 0.9815 - val_loss: 1.6318 - val_accuracy: 0.7095\n",
            "Epoch 50/50\n",
            "85/85 [==============================] - 14s 162ms/step - loss: 0.0413 - accuracy: 0.9904 - val_loss: 1.9263 - val_accuracy: 0.7140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8233a055f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}